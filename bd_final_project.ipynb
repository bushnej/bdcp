{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.functions import col, when, isnan, trim, count\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import QuantileDiscretizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"SparkTest\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# import data\n",
    "df = spark.read.csv(\"loan_1000.csv\", inferSchema = True, header = True)\n",
    "\n",
    "# fill na\n",
    "df = df.na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[2500.0,2500.0,25...|\n",
      "|[30000.0,30000.0,...|\n",
      "|(96,[0,1,2,3,4,5,...|\n",
      "|(96,[0,1,2,3,4,5,...|\n",
      "|(96,[0,1,2,3,4,5,...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create feature vector\n",
    "cols = df.columns\n",
    "cols.remove('funded_ratio')\n",
    "assembler = VectorAssembler(inputCols = cols, outputCol = 'features')\n",
    "\n",
    "# transform df with the newly created vectorassembler\n",
    "df = assembler.transform(df)\n",
    "df.select('features').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>funded_ratio_cat</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   funded_ratio_cat  count\n",
       "0               0.0     20\n",
       "1               1.0    979"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bucket dependent variable\n",
    "discretizer = QuantileDiscretizer(numBuckets = 3, inputCol = \"funded_ratio\", outputCol = \"funded_ratio_cat\")\n",
    "df = discretizer.fit(df).transform(df)\n",
    "df.groupby(\"funded_ratio_cat\").count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test\n",
    "train, test = df.randomSplit([.8,.2],seed = 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for Logistic Regression\n",
    "lgr = LogisticRegression(maxIter = 10, featuresCol = 'features', labelCol = 'funded_ratio_cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model to the data\n",
    "lgrm = lgr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a dataset, predict each point's label, and show the results.\n",
    "predict_train = lgrm.transform(train)\n",
    "predict_test = lgrm.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_confusion_matrix\n",
      " [[ 13.   3.]\n",
      " [  2. 808.]]\n",
      "train_accuracy 0.9939467312348669\n",
      "train_precision 0.8666666666666667\n",
      "trian_recall 0.8125\n"
     ]
    }
   ],
   "source": [
    "train_results = predict_train.select(['prediction', 'funded_ratio_cat'])\n",
    "train_predictionAndLabels = train_results.rdd\n",
    "train_metrics = MulticlassMetrics(train_predictionAndLabels)\n",
    "\n",
    "train_cm = train_metrics.confusionMatrix().toArray()\n",
    "trian_accuracy = (train_cm[0][0] + train_cm[1][1])/train_cm.sum()\n",
    "train_precision = (train_cm[0][0])/(train_cm[0][0] + train_cm[1][0])\n",
    "train_recall = (train_cm[0][0])/(train_cm[0][0]+train_cm[0][1])\n",
    "\n",
    "print(\"train_confusion_matrix\\n\",train_cm)\n",
    "print(\"train_accuracy\",trian_accuracy)\n",
    "print(\"train_precision\",train_precision)\n",
    "print(\"trian_recall\",train_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_confusion_matrix\n",
      " [[  0.   4.]\n",
      " [  0. 169.]]\n",
      "test_accuracy 0.976878612716763\n",
      "test_precision nan\n",
      "test_recall 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "test_results = predict_test.select(['prediction', 'funded_ratio_cat'])\n",
    "test_predictionAndLabels = test_results.rdd\n",
    "test_metrics = MulticlassMetrics(test_predictionAndLabels)\n",
    "\n",
    "test_cm = test_metrics.confusionMatrix().toArray()\n",
    "test_accuracy = (test_cm[0][0] + test_cm[1][1])/test_cm.sum()\n",
    "test_precision = (test_cm[0][0])/(test_cm[0][0] + test_cm[1][0])\n",
    "test_recall = (test_cm[0][0])/(test_cm[0][0] + test_cm[0][1])\n",
    "\n",
    "print(\"test_confusion_matrix\\n\",test_cm)\n",
    "print(\"test_accuracy\",test_accuracy)\n",
    "print(\"test_precision\",test_precision)\n",
    "print(\"test_recall\",test_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation metrics\n",
    "evaluator_train = MulticlassClassificationEvaluator(labelCol = 'funded_ratio_cat', predictionCol = 'predict_train')\n",
    "evaluator_test = MulticlassClassificationEvaluator(labelCol = 'funded_ratio_cat', predictionCol = 'predict_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Accuracy score for predicted train is {}\"\\\n",
    "#       .format(evaluator_train.evaluate(predict_train, {evaluator_train.metricName: \"accuracy\"})))\n",
    "# print(\"f1 score for predicted train is {}\"\\\n",
    "#       .format(evaluator_train.evaluate(predict_train, {evaluator_train.metricName: \"f1\"})))\n",
    "\n",
    "# print(\"\\nAccuracy score for predicted test is {}\"\\\n",
    "#       .format(evaluator.evaluate(predict_test, {evaluator.metricName: \"accuracy\"})))\n",
    "# print(\"f1 score for predicted test is {}\"\\\n",
    "#       .format(evaluator.evaluate(predict_test, {evaluator.metricName: \"f1\"})))\n",
    "\n",
    "# print(\"\\nThe area under ROC for train set is {}\".format(evaluator.evaluate(predict_train)))\n",
    "# print(\"The area under ROC for test set is {}\".format(evaluator.evaluate(predict_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
