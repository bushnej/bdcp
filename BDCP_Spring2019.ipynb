{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"SparkTest\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://md01.rcc.local:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0-cdh6.1.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f5c35e9a4a8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and join data\n",
    "dfLC = spark.read.csv(\"loan.csv\", inferSchema = True, header = True)\n",
    "dfLC = dfLC.withColumn(\"zip_code_3\", dfLC.zip_code.substr(1,3))\n",
    "\n",
    "dfZ = spark.read.csv(\"Zip_Zhvi_AllHomes.csv\", inferSchema = True, header = True)\n",
    "dfZ = dfZ.withColumn(\"zip_code_3_throwaway\", dfZ.RegionName.substr(1,3)) \n",
    "dfZ = dfZ.select([c for c in dfZ.columns if c not in {'id','member_id','zip_code','RegionName','RegionID','City','State','Metro','CountyName','SizeRank','1996-04','1996-05','1996-06','1996-07','1996-08','1996-09','1996-10','1996-11','1996-12','1997-01','1997-02','1997-03','1997-04','1997-05','1997-06','1997-07','1997-08','1997-09','1997-10','1997-11','1997-12','1998-01','1998-02','1998-03','1998-04','1998-05','1998-06','1998-07','1998-08','1998-09','1998-10','1998-11','1998-12','1999-01','1999-02','1999-03','1999-04','1999-05','1999-06','1999-07','1999-08','1999-09','1999-10','1999-11','1999-12','2000-01','2000-02','2000-03','2000-04','2000-05','2000-06','2000-07','2000-08','2000-09','2000-10','2000-11','2000-12','2001-01','2001-02','2001-03','2001-04','2001-05','2001-06','2001-07','2001-08','2001-09','2001-10','2001-11','2001-12','2002-01','2002-02','2002-03','2002-04','2002-05','2002-06','2002-07','2002-08','2002-09','2002-10','2002-11','2002-12','2003-01','2003-02','2003-03','2003-04','2003-05','2003-06','2003-07','2003-08','2003-09','2003-10','2003-11','2003-12','2004-01','2004-02','2004-03','2004-04','2004-05','2004-06','2004-07','2004-08','2004-09','2004-10','2004-11','2004-12','2005-01','2005-02','2005-03','2005-04','2005-05','2005-06','2005-07','2005-08','2005-09','2005-10','2005-11','2005-12','2006-01','2006-02','2006-03','2006-04','2006-05','2006-06','2006-07','2006-08','2006-09','2006-10','2006-11','2006-12','2007-01','2007-02','2007-03','2007-04','2007-05','2007-06','2007-07','2007-08','2007-09','2007-10','2007-11','2007-12','2008-01','2008-02','2008-03','2008-04','2008-05','2008-06','2008-07','2008-08','2008-09','2008-10','2008-11','2008-12','2009-01','2009-02','2009-03','2009-04','2009-05','2009-06','2009-07','2009-08','2009-09','2009-10','2009-11','2009-12','2010-01','2010-02','2010-03','2010-04','2010-05','2010-06','2010-07','2010-08','2010-09','2010-10','2010-11','2010-12','2011-01','2011-02','2011-03','2011-04','2011-05','2011-06','2011-07','2011-08','2011-09','2011-10','2011-11','2011-12','2012-01','2012-02','2012-03','2012-04','2012-05','2012-06','2012-07','2012-08','2012-09','2012-10','2012-11','2012-12','2013-01','2013-02','2013-03','2013-04','2013-05','2013-06','2013-07','2013-08','2013-09','2013-10','2013-11','2013-12','2014-01','2014-02','2014-03','2014-04','2014-05','2014-06','2014-07','2014-08','2014-09','2014-10','2014-11','2014-12','2015-01','2015-02','2015-03','2015-04','2015-05','2015-06','2015-07','2015-08','2015-09','2015-10','2015-11','2015-12','2016-01','2016-02','2016-03','2016-04','2016-05','2016-06','2016-07','2016-08','2016-09','2016-10','2016-11','2016-12','2017-01','2017-02','2017-03','2017-04','2017-05','2017-06','2017-07','2017-08','2017-09','2017-10','2017-11','2017-12','2018-01','2018-02','2018-03','2018-04','2018-05','2018-06','2018-07','2018-08','2018-09','2018-10','2018-11','2018-12','2019-01','2019-02'}]) \\\n",
    "           .withColumn(\"IndexValue\", dfZ[\"2019-03\"]) \\\n",
    "           .groupBy(\"zip_code_3_throwaway\").mean('IndexValue').collect()\n",
    "dfZ = spark.createDataFrame(data = dfZ, schema = ['zip_code_3_throwaway', 'IndexValueMean'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import substring\n",
    "data = dfLC.join(dfZ, dfLC.zip_code_3 == dfZ.zip_code_3_throwaway).select(dfLC[\"*\"],dfZ[\"*\"])\n",
    "data = data.select([c for c in data.columns if c not in {'id','member_id','zip_code','verification_status_joint','acc_now_delinq','hardship_type','hardship_reason','hardship_status','deferral_term','hardship_amount','hardship_start_date','hardship_end_date','payment_plan_start_date','hardship_length','hardship_dpd','hardship_loan_status','orig_projected_additional_accrued_interest','hardship_payoff_balance_amount','hardship_last_payment_amount','debt_settlement_flag','debt_settlement_flag_date','settlement_status','settlement_date','settlement_amount','settlement_percentage','settlement_term','hardship_flag','policy_code','out_prncp','out_prncp_inv','total_pymnt','total_pymnt_inv','total_rec_prncp','total_rec_int','total_rec_late_fee','recoveries','collection_recovery_fee','last_pymnt_d','last_pymnt_amnt','next_pymnt_d','zip_code_3_throwaway','desc','loan_status','pymnt_plan','url','int_rate','installment','grade','sub_grade'}]) \\\n",
    "           .withColumn(\"funded_ratio\", data.funded_amnt_inv / data.loan_amnt)\n",
    "data = data.withColumn('issue_year', substring('issue_d', 5, 7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA: find funded ratio and counts for each year\n",
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "\n",
    "data_orig = data\n",
    "discretizer = QuantileDiscretizer(numBuckets=2, inputCol=\"funded_ratio\", outputCol=\"funded_ratio_category\")\n",
    "data_orig = discretizer.fit(data_orig).transform(data_orig) \\\n",
    "                       .groupBy(['issue_year','funded_ratio_category']).count().toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>issue_year</th>\n",
       "      <th>funded_ratio_category</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2007</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018</td>\n",
       "      <td>1.0</td>\n",
       "      <td>435713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2009</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2013</td>\n",
       "      <td>1.0</td>\n",
       "      <td>105036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>375850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2015</td>\n",
       "      <td>1.0</td>\n",
       "      <td>354915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2012</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2011</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2008</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2014</td>\n",
       "      <td>1.0</td>\n",
       "      <td>199197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2017</td>\n",
       "      <td>1.0</td>\n",
       "      <td>389751</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   issue_year  funded_ratio_category   count\n",
       "0        2007                    0.0     482\n",
       "1        2007                    1.0       1\n",
       "2        2008                    0.0    2080\n",
       "3        2018                    1.0  435713\n",
       "4        2017                    0.0   12603\n",
       "5        2013                    0.0   17990\n",
       "6        2015                    0.0   29147\n",
       "7        2009                    1.0    1036\n",
       "8        2014                    0.0   15713\n",
       "9        2013                    1.0  105036\n",
       "10       2011                    0.0    6825\n",
       "11       2016                    1.0  375850\n",
       "12       2009                    0.0    3534\n",
       "13       2015                    1.0  354915\n",
       "14       2010                    1.0    4285\n",
       "15       2012                    1.0   39543\n",
       "16       2011                    1.0   12575\n",
       "17       2018                    0.0   14400\n",
       "18       2008                    1.0      41\n",
       "19       2012                    0.0    8781\n",
       "20       2014                    1.0  199197\n",
       "21       2016                    0.0   19072\n",
       "22       2010                    0.0    6693\n",
       "23       2017                    1.0  389751"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function for ohe hot encoding a field\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.sql.functions import col, split, lit\n",
    "\n",
    "def oheField(dataSet, fieldNameToEncode, suffixToAppend, aVocabSize):\n",
    "    arrayFieldName = fieldNameToEncode+\"_array\"\n",
    "    oheFieldName = fieldNameToEncode+\"_ohe\"\n",
    "    termVectorizer = CountVectorizer(inputCol=arrayFieldName, outputCol=oheFieldName, vocabSize=aVocabSize, minDF=1.0)\n",
    "    aData = dataSet.withColumn(fieldNameToEncode, concat(col(fieldNameToEncode), lit(suffixToAppend))) \\\n",
    "                   .withColumn(arrayFieldName, split(col(fieldNameToEncode),\" \"))\n",
    "    data_ohe = termVectorizer.fit(aData).transform(aData)\n",
    "    return data_ohe.select([c for c in data_ohe.columns if c not in {arrayFieldName,fieldNameToEncode}])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean and OHE emp_title\n",
    "from pyspark.sql.functions import lower, col\n",
    "from pyspark.sql.functions import concat\n",
    "data = data.withColumn('emp_title', lower(col('emp_title')))\n",
    "from pyspark.sql.functions import when, col, coalesce, array\n",
    "fill = \"\"\n",
    "data = data.withColumn('emp_title_filled', when(col(\"emp_title\").isNull(), fill).otherwise(col(\"emp_title\")))\n",
    "data = data.select([c for c in data.columns if c not in {'emp_title'}])\n",
    "data = oheField(data, \"emp_title_filled\", \"\", 3)\n",
    "\n",
    "#clean employment length\n",
    "from pyspark.sql.functions import *\n",
    "fill0 = \"0 years\"\n",
    "fill10 = \"10 years\"\n",
    "data = data.withColumn('emp_length', when(col(\"emp_length\")=='n/a', fill0).otherwise(col(\"emp_length\")))\n",
    "data = data.withColumn('emp_length', when(col(\"emp_length\")=='< 1 year', fill0).otherwise(col(\"emp_length\"))) \n",
    "data = data.withColumn('emp_length', when(col(\"emp_length\")=='10+ years', fill10).otherwise(col(\"emp_length\"))) \n",
    "data = data.withColumn('emp_length', regexp_replace('emp_length', ' years', '')) \n",
    "data = data.withColumn('emp_length', regexp_replace('emp_length', ' year', '')) \n",
    "data = data.withColumn(\"emp_length\", data[\"emp_length\"].cast(\"integer\"))\n",
    "\n",
    "#OHE more fields\n",
    "data = oheField(data, \"term\", \"\", 3)\n",
    "data = oheField(data, \"home_ownership\", \"\", 4) \n",
    "data = oheField(data, \"verification_status\", \"\", 4)\n",
    "data = oheField(data, \"purpose\", \"\", 4)\n",
    "data = oheField(data, \"addr_state\",\" State\",4)\n",
    "data = oheField(data, \"zip_code_3\", \" zip\", 3)\n",
    "\n",
    "#clean and OHE title\n",
    "fill = \"no title\"\n",
    "data = data.withColumn('title', when(col(\"title\").isNull(), fill).otherwise(col(\"title\")))\n",
    "data = oheField(data, \"title\", \"\", 4)\n",
    "data = oheField(data, \"application_type\",\" application\", 4)\n",
    "\n",
    "#convert datestamp earliest_cr_line\n",
    "fill = \"May-2019\"\n",
    "data = data.withColumn('earliest_cr_line', when(col(\"earliest_cr_line\").isNull(), fill).otherwise(col(\"earliest_cr_line\")))\n",
    "data = data.withColumn('earliest_cr_line', unix_timestamp('earliest_cr_line', 'MMM-yyy').alias('earliest_cr_line_date'))\n",
    "data = data.withColumn('issue_d', unix_timestamp('issue_d', 'MMM-yyy').alias('issue_d'))\n",
    "\n",
    "#convert datestamp initial_list_state\n",
    "fill = \"unknown\"\n",
    "data = data.withColumn('initial_list_status', when(col(\"initial_list_status\").isNull(), fill).otherwise(col(\"initial_list_status\")))\n",
    "data = oheField(data, \"initial_list_status\", \" state\", 4)\n",
    "data = data.withColumn('last_credit_pull_d', unix_timestamp('last_credit_pull_d', 'MMM-yyy').alias('last_credit_pull_d_date'))\n",
    "\n",
    "#clean and OHE disbursement_method\n",
    "fill = \"unknown\"\n",
    "data = data.withColumn('disbursement_method', when(col(\"disbursement_method\").isNull(), fill).otherwise(col(\"disbursement_method\")))\n",
    "data = oheField(data, \"disbursement_method\",\" method\", 10)\n",
    "\n",
    "#convert datestamp sec_app_earliest_cr_line\n",
    "data = data.withColumn('sec_app_earliest_cr_line', unix_timestamp('sec_app_earliest_cr_line', 'MMM-yyy').alias('sec_app_earliest_cr_line'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the data types on a group of fields\n",
    "data = data.withColumn(\"annual_inc\", data[\"annual_inc\"].cast(\"integer\")) \\\n",
    "  .withColumn(\"dti\", data[\"dti\"].cast(\"double\")) \\\n",
    "  .withColumn(\"delinq_2yrs\", data[\"delinq_2yrs\"].cast(\"integer\")) \\\n",
    "  .withColumn(\"inq_last_6mths\", data[\"inq_last_6mths\"].cast(\"integer\")) \\\n",
    "  .withColumn(\"mths_since_last_delinq\", data[\"mths_since_last_delinq\"].cast(\"integer\")) \\\n",
    "  .withColumn(\"mths_since_last_record\", data[\"mths_since_last_record\"].cast(\"integer\")) \\\n",
    "  .withColumn(\"open_acc\", data[\"open_acc\"].cast(\"integer\")) \\\n",
    "  .withColumn(\"pub_rec\", data[\"pub_rec\"].cast(\"integer\")) \\\n",
    "  .withColumn(\"revol_bal\", data[\"revol_bal\"].cast(\"integer\")) \\\n",
    "  .withColumn(\"revol_util\", data[\"revol_util\"].cast(\"integer\")) \\\n",
    "  .withColumn(\"total_acc\", data[\"total_acc\"].cast(\"integer\")) \\\n",
    "  .withColumn(\"collections_12_mths_ex_med\", data[\"collections_12_mths_ex_med\"].cast(\"integer\")) \\\n",
    "  .withColumn(\"mths_since_last_major_derog\", data[\"mths_since_last_major_derog\"].cast(\"integer\")) \\\n",
    "  .withColumn(\"annual_inc_joint\", data[\"annual_inc_joint\"].cast(\"integer\")) \\\n",
    "  .withColumn(\"dti_joint\", data[\"dti_joint\"].cast(\"double\")) \\\n",
    "  .withColumn(\"tot_coll_amt\", data[\"tot_coll_amt\"].cast(\"integer\")) \\\n",
    "  .withColumn(\"tot_cur_bal\", data[\"tot_cur_bal\"].cast(\"integer\")) \\\n",
    "  .withColumn(\"open_acc_6m\", data[\"open_acc_6m\"].cast(\"integer\")) \\\n",
    "  .withColumn(\"open_act_il\", data[\"open_act_il\"].cast(\"integer\")) \\\n",
    "  .withColumn(\"open_il_12m\", data[\"open_il_12m\"].cast(\"integer\")) \\\n",
    "  .withColumn(\"open_il_24m\", data[\"open_il_24m\"].cast(\"integer\")) \\\n",
    "  .withColumn(\"mths_since_rcnt_il\", data[\"mths_since_rcnt_il\"].cast(\"integer\")) \\\n",
    "  .withColumn(\"total_bal_il\", data[\"total_bal_il\"].cast(\"integer\")) \\\n",
    "  .withColumn(\"il_util\", data[\"il_util\"].cast(\"integer\")) \\\n",
    "  .withColumn(\"open_rv_12m\", data[\"open_rv_12m\"].cast(\"integer\")) \\\n",
    "  .withColumn(\"open_rv_24m\", data[\"open_rv_24m\"].cast(\"integer\")) \\\n",
    "  .withColumn(\"max_bal_bc\", data[\"max_bal_bc\"].cast(\"integer\")) \\\n",
    "  .withColumn(\"all_util\", data[\"all_util\"].cast(\"integer\")) \\\n",
    "  .withColumn(\"total_rev_hi_lim\", data[\"total_rev_hi_lim\"].cast(\"integer\")) \\\n",
    "  .withColumn(\"inq_fi\", data[\"inq_fi\"].cast(\"integer\")) \\\n",
    "  .withColumn(\"total_cu_tl\", data[\"total_cu_tl\"].cast(\"integer\")) \\\n",
    "  .withColumn(\"inq_last_12m\", data[\"inq_last_12m\"].cast(\"integer\")) \\\n",
    "  .withColumn(\"avg_cur_bal\", data[\"avg_cur_bal\"].cast(\"integer\")) \\\n",
    "  .withColumn(\"bc_util\", data[\"bc_util\"].cast(\"integer\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, VectorAssembler, IndexToString, StringIndexer, VectorIndexer, QuantileDiscretizer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator \n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "dataRF = data\n",
    "dataRF = dataRF.na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = dataRF.columns\n",
    "cols.remove(\"funded_ratio\")\n",
    "cols.remove(\"issue_year\")\n",
    "#import vector assembler\n",
    "assembler = VectorAssembler(inputCols=cols,outputCol=\"features\")\n",
    "#transform data\n",
    "dataRF = assembler.transform(dataRF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Yes/No\n",
    "discretizer = QuantileDiscretizer(numBuckets=2, inputCol=\"funded_ratio\", outputCol=\"funded_ratio_category\")\n",
    "dataRF = discretizer.fit(dataRF).transform(dataRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------+\n",
      "|funded_ratio_category|  count|\n",
      "+---------------------+-------+\n",
      "|                  0.0| 137320|\n",
      "|                  1.0|1917943|\n",
      "+---------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataRF.groupBy(\"funded_ratio_category\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 1644008\n",
      "Test Dataset Count: 411255\n"
     ]
    }
   ],
   "source": [
    "#Split train and test data by 80/20\n",
    "train, test = dataRF.randomSplit([0.8, 0.2], seed = 1234)\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Classifier\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'funded_ratio_category')\n",
    "rfModel = rf.fit(train)\n",
    "predict_train = rfModel.transform(train)\n",
    "predict_test = rfModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------------------+----------+--------------------+\n",
      "|funded_ratio_category|       rawPrediction|prediction|         probability|\n",
      "+---------------------+--------------------+----------+--------------------+\n",
      "|                  0.0|[7.59890796595921...|       1.0|[0.37994539829796...|\n",
      "|                  0.0|[7.71059800006007...|       1.0|[0.38552990000300...|\n",
      "|                  0.0|[1.44481207589672...|       1.0|[0.07224060379483...|\n",
      "|                  1.0|[0.93908315287707...|       1.0|[0.04695415764385...|\n",
      "|                  1.0|[0.98692946432570...|       1.0|[0.04934647321628...|\n",
      "|                  1.0|[0.88672144111224...|       1.0|[0.04433607205561...|\n",
      "|                  1.0|[0.9985545444098,...|       1.0|[0.04992772722049...|\n",
      "|                  1.0|[1.44481207589672...|       1.0|[0.07224060379483...|\n",
      "|                  1.0|[0.9985545444098,...|       1.0|[0.04992772722049...|\n",
      "|                  1.0|[0.9985545444098,...|       1.0|[0.04992772722049...|\n",
      "+---------------------+--------------------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "The area under ROC for train set is 0.91018156487342\n",
      "The area under ROC for test set is 0.9100144947810961\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol= \"funded_ratio_category\")\n",
    "predict_test.select(\"funded_ratio_category\",\"rawPrediction\",\"prediction\",\"probability\").show(10)\n",
    "print(\"The area under ROC for train set is {}\".format(evaluator.evaluate(predict_train)))\n",
    "print(\"The area under ROC for test set is {}\".format(evaluator.evaluate(predict_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test CM [[  1758.  25636.]\n",
      " [   841. 383020.]]\n",
      "Test accuracy 0.9356190198295461\n",
      "Test precision 0.6764140053866872\n",
      "Test recall 0.06417463678177703\n"
     ]
    }
   ],
   "source": [
    "results = predict_test.select(['prediction', 'funded_ratio_category'])\n",
    "predictionAndLabels=results.rdd\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "cm=metrics.confusionMatrix().toArray()\n",
    "accuracy=(cm[0][0]+cm[1][1])/cm.sum()\n",
    "precision=(cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "recall=(cm[0][0])/(cm[0][0]+cm[0][1])\n",
    "\n",
    "\n",
    "print(\"Test CM\",cm)\n",
    "print(\"Test accuracy\",accuracy)\n",
    "print(\"Test precision\",precision)\n",
    "print(\"Test recall\",recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 0.11722735304904476\n"
     ]
    }
   ],
   "source": [
    "print(\"Test F1\",(precision*recall)/(precision+recall)*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "attrs = sorted(\n",
    "    (attr[\"idx\"], attr[\"name\"]) for attr in (chain(*predict_train\n",
    "        .schema[\"features\"]\n",
    "        .metadata[\"ml_attr\"][\"attrs\"].values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureImportance = [(name, rfModel.featureImportances[idx])\n",
    " for idx, name in attrs\n",
    " if rfModel.featureImportances[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>columnName</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>initial_list_status_ohe_1</td>\n",
       "      <td>0.128867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>tot_cur_bal</td>\n",
       "      <td>0.108727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>pct_tl_nvr_dlq</td>\n",
       "      <td>0.089927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>num_op_rev_tl</td>\n",
       "      <td>0.081740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>num_bc_tl</td>\n",
       "      <td>0.075052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>total_bal_ex_mort</td>\n",
       "      <td>0.058599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>num_actv_bc_tl</td>\n",
       "      <td>0.052565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>num_sats</td>\n",
       "      <td>0.045876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>issue_d</td>\n",
       "      <td>0.042335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>total_rev_hi_lim</td>\n",
       "      <td>0.037052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>loan_amnt</td>\n",
       "      <td>0.034478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>num_rev_accts</td>\n",
       "      <td>0.031316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>total_bc_limit</td>\n",
       "      <td>0.024457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>num_bc_sats</td>\n",
       "      <td>0.022374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>mo_sin_rcnt_rev_tl_op</td>\n",
       "      <td>0.020229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>num_il_tl</td>\n",
       "      <td>0.018866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>acc_open_past_24mths</td>\n",
       "      <td>0.016230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>bc_util</td>\n",
       "      <td>0.015818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>total_cu_tl</td>\n",
       "      <td>0.011803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>bc_open_to_buy</td>\n",
       "      <td>0.011734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>max_bal_bc</td>\n",
       "      <td>0.011525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>num_actv_rev_tl</td>\n",
       "      <td>0.007504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>avg_cur_bal</td>\n",
       "      <td>0.007470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>funded_amnt_inv</td>\n",
       "      <td>0.007224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>mths_since_recent_bc</td>\n",
       "      <td>0.006297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>num_tl_op_past_12m</td>\n",
       "      <td>0.006270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>verification_status_ohe_1</td>\n",
       "      <td>0.004089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>mths_since_recent_inq</td>\n",
       "      <td>0.003423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>tot_hi_cred_lim</td>\n",
       "      <td>0.002870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>initial_list_status_ohe_2</td>\n",
       "      <td>0.002269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>earliest_cr_line</td>\n",
       "      <td>0.001341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>term_ohe_2</td>\n",
       "      <td>0.000960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>mo_sin_old_rev_tl_op</td>\n",
       "      <td>0.000899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>annual_inc</td>\n",
       "      <td>0.000885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>mo_sin_rcnt_tl</td>\n",
       "      <td>0.000763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>revol_util</td>\n",
       "      <td>0.000459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mths_since_last_delinq</td>\n",
       "      <td>0.000269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>mo_sin_old_il_acct</td>\n",
       "      <td>0.000259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>percent_bc_gt_75</td>\n",
       "      <td>0.000247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>sec_app_open_act_il</td>\n",
       "      <td>0.000226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>last_credit_pull_d</td>\n",
       "      <td>0.000215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>mort_acc</td>\n",
       "      <td>0.000173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>home_ownership_ohe_0</td>\n",
       "      <td>0.000159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>open_acc</td>\n",
       "      <td>0.000157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>delinq_2yrs</td>\n",
       "      <td>0.000071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>disbursement_method_ohe_2</td>\n",
       "      <td>0.000042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>all_util</td>\n",
       "      <td>0.000041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>chargeoff_within_12_mths</td>\n",
       "      <td>0.000024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>emp_length</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>sec_app_mort_acc</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>zip_code_3_ohe_2</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>disbursement_method_ohe_1</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>home_ownership_ohe_2</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>open_rv_12m</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pub_rec</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>mths_since_recent_bc_dlq</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>sec_app_chargeoff_within_12_mths</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>pub_rec_bankruptcies</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>total_bal_il</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>open_rv_24m</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          columnName  importance\n",
       "59         initial_list_status_ohe_1    0.128867\n",
       "15                       tot_cur_bal    0.108727\n",
       "45                    pct_tl_nvr_dlq    0.089927\n",
       "41                     num_op_rev_tl    0.081740\n",
       "39                         num_bc_tl    0.075052\n",
       "49                 total_bal_ex_mort    0.058599\n",
       "36                    num_actv_bc_tl    0.052565\n",
       "43                          num_sats    0.045876\n",
       "5                            issue_d    0.042335\n",
       "21                  total_rev_hi_lim    0.037052\n",
       "0                          loan_amnt    0.034478\n",
       "42                     num_rev_accts    0.031316\n",
       "50                    total_bc_limit    0.024457\n",
       "38                       num_bc_sats    0.022374\n",
       "30             mo_sin_rcnt_rev_tl_op    0.020229\n",
       "40                         num_il_tl    0.018866\n",
       "23              acc_open_past_24mths    0.016230\n",
       "26                           bc_util    0.015818\n",
       "22                       total_cu_tl    0.011803\n",
       "25                    bc_open_to_buy    0.011734\n",
       "19                        max_bal_bc    0.011525\n",
       "37                   num_actv_rev_tl    0.007504\n",
       "24                       avg_cur_bal    0.007470\n",
       "2                    funded_amnt_inv    0.007224\n",
       "33              mths_since_recent_bc    0.006297\n",
       "44                num_tl_op_past_12m    0.006270\n",
       "57         verification_status_ohe_1    0.004089\n",
       "35             mths_since_recent_inq    0.003423\n",
       "48                   tot_hi_cred_lim    0.002870\n",
       "60         initial_list_status_ohe_2    0.002269\n",
       "..                               ...         ...\n",
       "7                   earliest_cr_line    0.001341\n",
       "54                        term_ohe_2    0.000960\n",
       "29              mo_sin_old_rev_tl_op    0.000899\n",
       "4                         annual_inc    0.000885\n",
       "31                    mo_sin_rcnt_tl    0.000763\n",
       "13                        revol_util    0.000459\n",
       "9             mths_since_last_delinq    0.000269\n",
       "28                mo_sin_old_il_acct    0.000259\n",
       "46                  percent_bc_gt_75    0.000247\n",
       "52               sec_app_open_act_il    0.000226\n",
       "14                last_credit_pull_d    0.000215\n",
       "32                          mort_acc    0.000173\n",
       "55              home_ownership_ohe_0    0.000159\n",
       "10                          open_acc    0.000157\n",
       "6                        delinq_2yrs    0.000071\n",
       "62         disbursement_method_ohe_2    0.000042\n",
       "20                          all_util    0.000041\n",
       "27          chargeoff_within_12_mths    0.000024\n",
       "3                         emp_length    0.000020\n",
       "51                  sec_app_mort_acc    0.000016\n",
       "58                  zip_code_3_ohe_2    0.000012\n",
       "61         disbursement_method_ohe_1    0.000009\n",
       "56              home_ownership_ohe_2    0.000009\n",
       "17                       open_rv_12m    0.000008\n",
       "11                           pub_rec    0.000007\n",
       "34          mths_since_recent_bc_dlq    0.000006\n",
       "53  sec_app_chargeoff_within_12_mths    0.000006\n",
       "47              pub_rec_bankruptcies    0.000006\n",
       "16                      total_bal_il    0.000005\n",
       "18                       open_rv_24m    0.000005\n",
       "\n",
       "[63 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureImportance = pd.DataFrame.from_records(featureImportance, columns=['columnName','importance'])\n",
    "featureImportance.sort_values(by='importance', ascending=False, inplace=True)\n",
    "featureImportance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "featureImportance.set_index('columnName').plot.barh(figsize=(10,10))#title='Feature Importance', figsize=(8,8))\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.functions import col, when, isnan, trim, count\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import QuantileDiscretizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "logModelStage = LogisticRegression(maxIter = 10, featuresCol = 'features', labelCol = 'funded_ratio_cat', weightCol = 'weightCol')\n",
    "somestages = [logModelStage]\n",
    "\n",
    "def calculateResults(trainOrTest, modelTransform):\n",
    "    aResults = modelTransform.select(['prediction', 'funded_ratio_cat'])\n",
    "    aPredictionAndLabels = aResults.rdd\n",
    "    aMetrics = MulticlassMetrics(aPredictionAndLabels)\n",
    "    aCM = aMetrics.confusionMatrix().toArray()\n",
    "    anAccuracy = (aCM[0][0] + aCM[1][1])/aCM.sum()\n",
    "    aPrecision = (aCM[0][0])/(aCM[0][0] + aCM[1][0])\n",
    "    aRecall = (aCM[0][0])/(aCM[0][0]+aCM[0][1])\n",
    "    print(trainOrTest,\"confusion_matrix\\n\",aCM)\n",
    "    print(trainOrTest,\"accuracy\",anAccuracy)\n",
    "    print(trainOrTest,\"precision\",aPrecision)\n",
    "    print(trainOrTest,\"recall\",aRecall)\n",
    "    print(trainOrTest,\"F1\",(aPrecision*aRecall)/(aPrecision+aRecall)*2)\n",
    "\n",
    "aPipeline = Pipeline(stages=somestages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prep unbalanced data train and test\n",
    "dataLog = data\n",
    "dataLog = dataLog.na.fill(0)\n",
    "cols = dataLog.columns\n",
    "cols.remove(\"funded_ratio\")\n",
    "cols.remove(\"issue_year\")\n",
    "#import vector assembler\n",
    "assembler = VectorAssembler(inputCols=cols,outputCol=\"features\")\n",
    "#transform data\n",
    "dataLog = assembler.transform(dataLog)\n",
    "# bucket dependent variable\n",
    "discretizer = QuantileDiscretizer(numBuckets = 2, inputCol = \"funded_ratio\", outputCol = \"funded_ratio_cat\")\n",
    "dataLog = discretizer.fit(dataLog).transform(dataLog)\n",
    "#add dummy weight column\n",
    "dataLog = dataLog.withColumn('weightCol', when(col(\"funded_ratio_cat\")>=0, 1).otherwise(1))\n",
    "# split data into train and test\n",
    "train, test = dataLog.randomSplit([.8,.2],seed = 1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train confusion_matrix\n",
      " [[  12814.   97112.]\n",
      " [  11992. 1522090.]]\n",
      "Train accuracy 0.9336353594386402\n",
      "Train precision 0.5165685721196485\n",
      "Train recall 0.11656932845732584\n",
      "Train F1 0.1902146483389247\n",
      "Test confusion_matrix\n",
      " [[  3135.  24259.]\n",
      " [  3035. 380826.]]\n",
      "Test accuracy 0.9336324178429442\n",
      "Test precision 0.5081037277147488\n",
      "Test recall 0.11444111849310068\n",
      "Test F1 0.18680729352878084\n"
     ]
    }
   ],
   "source": [
    "aPipelineFit = aPipeline.fit(train)\n",
    "aPipelineResult = aPipelineFit.transform(train)\n",
    "calculateResults(\"Train\", aPipelineResult)\n",
    "aPipelineResult = aPipelineFit.transform(test)\n",
    "calculateResults(\"Test\", aPipelineResult)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   funded_ratio_cat    count\n",
      "0               0.0   137320\n",
      "1               1.0  1917943\n"
     ]
    }
   ],
   "source": [
    "#Prep balanced data train and test\n",
    "dataLogBal = data\n",
    "dataLogBal = dataLogBal.na.fill(0)\n",
    "cols = dataLogBal.columns\n",
    "cols.remove(\"funded_ratio\")\n",
    "cols.remove(\"issue_year\")\n",
    "#import vector assembler\n",
    "assembler = VectorAssembler(inputCols=cols,outputCol=\"features\")\n",
    "#transform data\n",
    "dataLogBal = assembler.transform(dataLogBal)\n",
    "# bucket dependent variable\n",
    "discretizer = QuantileDiscretizer(numBuckets = 2, inputCol = \"funded_ratio\", outputCol = \"funded_ratio_cat\")\n",
    "dataLogBal = discretizer.fit(dataLogBal).transform(dataLogBal)\n",
    "print(dataLogBal.groupby(\"funded_ratio_cat\").count().toPandas())\n",
    "#weight needs to be increased on rows where funded_ratio_category = 0\n",
    "#Each row needs a weight of:\n",
    "fill = 1917943/137320\n",
    "dataLogBal = dataLogBal.withColumn('weightCol', when(col(\"funded_ratio_cat\")==0, fill).otherwise(1))\n",
    "# split data into train and test\n",
    "train, test = dataLogBal.randomSplit([.8,.2],seed = 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train confusion_matrix\n",
      " [[  98615.   11311.]\n",
      " [ 409196. 1124886.]]\n",
      "Train accuracy 0.7442183979640001\n",
      "Train precision 0.19419626593358552\n",
      "Train recall 0.897103505994942\n",
      "Train F1 0.31927826890731814\n",
      "Test confusion_matrix\n",
      " [[ 24637.   2757.]\n",
      " [102266. 281595.]]\n",
      "Test accuracy 0.7446280288385552\n",
      "Test precision 0.19414040645217212\n",
      "Test recall 0.8993575235453019\n",
      "Test F1 0.3193451590115168\n"
     ]
    }
   ],
   "source": [
    "aPipelineFit = aPipeline.fit(train)\n",
    "aPipelineResult = aPipelineFit.transform(train)\n",
    "calculateResults(\"Train\", aPipelineResult)\n",
    "aPipelineResult = aPipelineFit.transform(test)\n",
    "calculateResults(\"Test\", aPipelineResult)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prep balanced data train and test\n",
    "dataLogBal = data\n",
    "dataLogBal = dataLogBal.withColumn(\"issue_year\", dataLogBal[\"issue_year\"].cast(\"integer\"))\n",
    "dataLogBal = dataLogBal.na.fill(0)\n",
    "\n",
    "dataLogBal2008_2009 = dataLogBal.filter(dataLogBal.issue_year >= 2008).filter(dataLogBal.issue_year <= 2009)\n",
    "dataLogBal2010_2012 = dataLogBal.filter(dataLogBal.issue_year >= 2010).filter(dataLogBal.issue_year <= 2012)\n",
    "dataLogBal2013_2015 = dataLogBal.filter(dataLogBal.issue_year >= 2013).filter(dataLogBal.issue_year <= 2015)\n",
    "dataLogBal2016_2018 = dataLogBal.filter(dataLogBal.issue_year >= 2016).filter(dataLogBal.issue_year <= 2018)\n",
    "\n",
    "def prepDataLogBal(aDataSet):\n",
    "    cols = aDataSet.columns\n",
    "    cols.remove(\"funded_ratio\")\n",
    "    cols.remove(\"issue_year\")\n",
    "    #import vector assembler\n",
    "    assembler = VectorAssembler(inputCols=cols,outputCol=\"features\")\n",
    "    #transform data\n",
    "    aDataSet = assembler.transform(aDataSet)\n",
    "    # bucket dependent variable\n",
    "    discretizer = QuantileDiscretizer(numBuckets = 2, inputCol = \"funded_ratio\", outputCol = \"funded_ratio_cat\")\n",
    "    aDataSet = discretizer.fit(aDataSet).transform(aDataSet)\n",
    "    aRatio = aDataSet.groupby(\"funded_ratio_cat\").count().toPandas()\n",
    "    aRatio = spark.createDataFrame(data = aRatio, schema = ['cat', 'count'])\n",
    "    zeroCount = aRatio.select('count').filter(aRatio.cat==0).head()[0]\n",
    "    oneCount = aRatio.select('count').filter(aRatio.cat==1).head()[0]\n",
    "    aRatio = oneCount/zeroCount\n",
    "    #weight needs to be increased on rows where funded_ratio_category = 0\n",
    "    #Each row needs a weight of:\n",
    "    fill = aRatio\n",
    "    aDataSet = aDataSet.withColumn('weightCol', when(col(\"funded_ratio_cat\")==0, fill).otherwise(1))\n",
    "    # split data into train and test\n",
    "    train, test = aDataSet.randomSplit([.8,.2],seed = 1234)\n",
    "    return train, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train confusion_matrix\n",
      " [[2364.  314.]\n",
      " [ 637. 2047.]]\n",
      "Train accuracy 0.8226408056695262\n",
      "Train precision 0.7877374208597134\n",
      "Train recall 0.8827483196415236\n",
      "Train F1 0.832540940306392\n",
      "Test confusion_matrix\n",
      " [[583.  85.]\n",
      " [155. 506.]]\n",
      "Test accuracy 0.8194130925507901\n",
      "Test precision 0.7899728997289973\n",
      "Test recall 0.8727544910179641\n",
      "Test F1 0.8293029871977241\n"
     ]
    }
   ],
   "source": [
    "train, test = prepDataLogBal(dataLogBal2008_2009)\n",
    "aPipelineFit = aPipeline.fit(train)\n",
    "aPipelineResult = aPipelineFit.transform(train)\n",
    "calculateResults(\"Train\", aPipelineResult)\n",
    "aPipelineResult = aPipelineFit.transform(test)\n",
    "calculateResults(\"Test\", aPipelineResult)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train confusion_matrix\n",
      " [[13324.  4579.]\n",
      " [18194. 26965.]]\n",
      "Train accuracy 0.6388791982493419\n",
      "Train precision 0.42274255980709435\n",
      "Train recall 0.7442328101435514\n",
      "Train F1 0.5392039821128669\n",
      "Test confusion_matrix\n",
      " [[3294. 1102.]\n",
      " [4491. 6753.]]\n",
      "Test accuracy 0.6423913043478261\n",
      "Test precision 0.423121387283237\n",
      "Test recall 0.7493175614194723\n",
      "Test F1 0.5408422953780477\n"
     ]
    }
   ],
   "source": [
    "train, test = prepDataLogBal(dataLogBal2010_2012)\n",
    "aPipelineFit = aPipeline.fit(train)\n",
    "aPipelineResult = aPipelineFit.transform(train)\n",
    "calculateResults(\"Train\", aPipelineResult)\n",
    "aPipelineResult = aPipelineFit.transform(test)\n",
    "calculateResults(\"Test\", aPipelineResult)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train confusion_matrix\n",
      " [[ 38368.  11932.]\n",
      " [171318. 356333.]]\n",
      "Train accuracy 0.6829315980074435\n",
      "Train precision 0.18297835811642169\n",
      "Train recall 0.7627833001988071\n",
      "Train F1 0.2951543544652404\n",
      "Test confusion_matrix\n",
      " [[ 9544.  3006.]\n",
      " [42863. 88634.]]\n",
      "Test accuracy 0.6815692100494978\n",
      "Test precision 0.18211307649741446\n",
      "Test recall 0.7604780876494024\n",
      "Test F1 0.2938559354649999\n"
     ]
    }
   ],
   "source": [
    "train, test = prepDataLogBal(dataLogBal2013_2015)\n",
    "aPipelineFit = aPipeline.fit(train)\n",
    "aPipelineResult = aPipelineFit.transform(train)\n",
    "calculateResults(\"Train\", aPipelineResult)\n",
    "aPipelineResult = aPipelineFit.transform(test)\n",
    "calculateResults(\"Test\", aPipelineResult)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train confusion_matrix\n",
      " [[ 33484.   3492.]\n",
      " [162455. 798785.]]\n",
      "Train accuracy 0.8337564214558773\n",
      "Train precision 0.17088991982198543\n",
      "Train recall 0.9055603634790134\n",
      "Train F1 0.2875211987205633\n",
      "Test confusion_matrix\n",
      " [[  8259.    840.]\n",
      " [ 40313. 199761.]]\n",
      "Test accuracy 0.8348416561987053\n",
      "Test precision 0.1700362348678251\n",
      "Test recall 0.9076821628750412\n",
      "Test F1 0.2864177836347558\n"
     ]
    }
   ],
   "source": [
    "train, test = prepDataLogBal(dataLogBal2016_2018)\n",
    "aPipelineFit = aPipeline.fit(train)\n",
    "aPipelineResult = aPipelineFit.transform(train)\n",
    "calculateResults(\"Train\", aPipelineResult)\n",
    "aPipelineResult = aPipelineFit.transform(test)\n",
    "calculateResults(\"Test\", aPipelineResult)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark 4G 4e",
   "language": "python",
   "name": "pyspark2_4g"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
